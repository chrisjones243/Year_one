{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "focal-invite",
   "metadata": {},
   "source": [
    "## Naïve Bayes – Additional Advice\n",
    "This notebook guides you through multiple steps you can follow to create a naïve Bayes classifier if you wish to implement this classifier for the assignment. After following these steps you will still need to collate and move your code into the main assignment notebook file so that it meets the required format.\n",
    "\n",
    "Read each step (including the maths!) carefully.\n",
    "\n",
    "You can implement a naïve Bayes classifier without following this advice.\n",
    "\n",
    "**Note!** This notebook guides you only through the compulsory spam filtering part of the assignment. If you wish to do the optional part of the assignment on feature engineering for digit classification, you should be able to reuse this code for your classifier for this task. In this case, you should make sure the code is generalisable to more than 2 classes and arbitrary number of features.  \n",
    "\n",
    "This notebook will not be graded and **does not need to be submitted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advised-torture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam training data set: (1000, 55)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_spam = np.loadtxt(open(\"training_spam.csv\"), delimiter=\",\")\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-class",
   "metadata": {},
   "source": [
    "## The model:  naïve Bayes\n",
    "Your [naïve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier will distinguish between two classes:\n",
    "\n",
    "* $C = 1$ for spam messages\n",
    "* $C = 0$ for ham messages\n",
    "\n",
    "\n",
    "The classifier builds a model for the probability $p(C=c\\ |\\ \\text{message})$ that a given message belongs to a certain class. A new message is then classified based on the Bayesian *maximum a posteriori* estimate\n",
    "$\\require{color}$\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\  \\textcolor{blue}{p(C=c\\ |\\ \\text{message})}.\n",
    "\\end{equation}\n",
    "Using Bayes' rule we can write\n",
    "\n",
    "\\begin{equation}\n",
    "p(C=c\\ |\\ \\text{message}) = \\frac{p(\\text{message}\\ |\\ C=c)p(C=c)}{p(\\text{message}\\ |\\ C=1)p(C=1) + p(\\text{message}\\ |\\ C=0)p(C=0)}.  \\quad \\quad \n",
    "\\end{equation}\n",
    "\n",
    "The denominator is the same for both classes and we can thus drop it as it does not influence which class would get the maximum $\\textcolor{blue}{p(C=c\\ |\\ \\text{message})}$. This gives us\n",
    "\n",
    "\\begin{equation}\n",
    "\\textcolor{blue}{p(C=c\\ |\\ \\text{message})} \\propto \\textcolor{orange}{p(\\text{message}\\ |\\ C=c)}\\textcolor{green}{p(C=c)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\propto$ means \"proportional to\". I.e., the above is equivalent to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textcolor{blue}{p(C=c\\ |\\ \\text{message})} = Const * \\textcolor{orange}{p(\\text{message}\\ |\\ C=c)}\\textcolor{green}{p(C=c)},\n",
    "\\end{equation}\n",
    "\n",
    "where $Const$ stands for some constant value.\n",
    "\n",
    "The class priors $\\textcolor{green}{p(C=c)}$ can be computed directly (you will do so in exercise A) but we need to further simplify $\\textcolor{orange}{p(\\text{message} \\ |\\ C=c)}$.\n",
    "\n",
    "\n",
    "### Choice of the event model: *Multinomial* naïve Bayes\n",
    "\n",
    "Different naïve Bayes models differ in their distributional assumptions about $\\textcolor{orange}{p(\\text{message}\\ |\\ C=c)}$. We represent a message using a **binary** [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. Specifically, a message is represented as a set of $k$ keywords, that is, $message = (w_1, ..., w_k)$, where $w_i = 1$ if the  keyword $w_i$ appears in the message and $w_i = 0$ otherwise. In this particular assignment we have $k=54$ keywords.\n",
    "\n",
    "We assume that the $p(w_1, ..., w_k |\\ C=c)$ follows a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) for each class. Don't let the name scare you, this model simply assigns probabilities to different counts of events with multiple outcomes. So for example: \"I roll a biased six-sided die six times, what is the probability that I get each side occurring exactly once\" is a question that can be answered with a multinomial distribution. You don't need to understand all of the equations on the Wikipedia page.\n",
    "\n",
    "Since this is a naïve Bayes model, we assume conditional indepence of keywords given the class, i.e., $p(w_1, ..., w_k |\\ C=c) = \\prod_{i=1}^k p(w_i |\\ C=c)$. Intuitively, we assume that the words of a message were \"drawn\" independently from a bag of $k$ different words. Depending on the class membership $c$, each keyword $w$ has a probability $\\theta_{c, w}$ of being drawn. For example,\n",
    "\n",
    "* $\\theta_{spam, w}$ will have high value for $w \\in \\{$bank, transfer, buy,... $\\}$.\n",
    "* $\\theta_{ham, w}$ will have high value for $w \\in \\{$paper, conference, proposal, experiment,... $\\}$, if the training data was mostly gathered from emails of researchers.\n",
    "\n",
    "Under these assumptions, the probability of a whole message (it is also called *likelihood*), given that it belongs to class $c$, is then proportional to\n",
    "\\begin{equation}\n",
    "\\textcolor{orange}{p(\\text{message}\\ |\\ C=c)} \\propto \\prod_{i = 1}^k  \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)^{w_i}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The parameters $\\textcolor{brown}{\\theta_{c, w}}$ are estimated by counting the relative frequencies in the training data. Use **Laplace-smoothing** with $\\alpha = 1$ (add-one smoothing), that is,\n",
    "\\begin{equation}\n",
    "\\textcolor{brown}{\\theta_{c, w}} = \\frac{n_{c, w} + \\alpha}{n_{c} + k \\alpha},\n",
    "\\end{equation}\n",
    "where $n_{c, w}$ is the number of times the keyword $w$ appears in messages of class $c$ in the training set and $n_{c}$ is the total count of keywords for all messages of class $c$, that is, $n_{c} = \\sum_w n_{c, w}$.\n",
    "\n",
    "\n",
    "\n",
    "We are now finally able to rewrite the *maximum a posteriori* estimate in a form that is easy to compute:\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\ \\left[ \\textcolor{green}{p(C=c)}   \\prod_{i = 1}^k  \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)^{w_i}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Increasing numerical stability\n",
    "We can increase the numerical stability of the algorithm by taking logarithms of the posterior distributions. We can leverage this because the logarithm function does not change the position of the maximum of the input function, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{x}{\\operatorname{argmax}}\\log (f(x)) = \\underset{x}{\\operatorname{argmax}} f(x)\n",
    "\\end{equation}\n",
    "\n",
    "for any function $f(x)$. \n",
    "\n",
    "Probability computation often involves taking a product of many terms (as in our case we need to take a product of $k$ terms of keyword probabilities and the prior probability of the class). Each term would be less than 1 as it represent some probability. When we multiply several terms less than 1, we can end up dealing with very small numbers close to 0, which is not a good idea when processing numbers on a computer. Using the logarithm in this case helps in two folds: (i) the logarithm of a product is a sum of the logarithms, i.e., we are dealing with summation of numbers rather than multiplication, which is much easier from the numerical stability point of view, (ii) the logarithm converts probabilities that are numbers from $[0, 1]$ into numbers from $(-\\infty, 0]$, so that we do not process very small numbers close to 0.\n",
    "\n",
    "This trick of using the logarithm of probabilities instead of probabilities themselves is often used in probabilistic modelling.\n",
    "\n",
    "In our case, it results as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\ \\log \\left( \\textcolor{green}{p(C=c)}   \\prod_{i = 1}^k  \\left(\\textcolor{brown}{\\theta_{c, w_i}}\\right)^{w_i} \\right) \\\\\n",
    " = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\ \\left[ \\log( \\textcolor{green}{p(C=c)}) + \\sum_{i = 1}^k w_i \\ \\log \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right) \\right].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-serial",
   "metadata": {},
   "source": [
    "## Part A: Estimate class priors\n",
    "\n",
    "Note that for defining the empirical class priors we do not need input features, but only response variable. \n",
    "\n",
    "Define a function called `estimate_log_class_priors()` that takes as input a data set with binary response variable (0s and 1s) and returns a numpy array containing the **logarithm** of the empirical class priors $\\textcolor{green}{p(C=c)}$ for $c \\in \\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "selected-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_log_class_priors(data):\n",
    "    \"\"\"\n",
    "    Given a data set with binary response variable (0s and 1s), \n",
    "    calculate the logarithm of the empirical class priors,\n",
    "    that is, the logarithm of the proportions of 0s and 1s:\n",
    "        log(p(C=0)) and log(p(C=1))\n",
    "\n",
    "    :param data: a numpy array of length n_samples\n",
    "                 that contains the binary response (coded as 0s and 1s).\n",
    "\n",
    "    :return log_class_priors: a numpy array of length two\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE...\n",
    "    log_class_priors = np.zeros(2)\n",
    "    log_class_priors[0] = np.log(np.sum(data == 0) / len(data))\n",
    "    log_class_priors[1] = np.log(np.sum(data == 1) / len(data))\n",
    "    return log_class_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "silver-miniature",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50e2bb5ceadb6ebc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result [-0.48939034 -0.94933059]\n"
     ]
    }
   ],
   "source": [
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_priors = estimate_log_class_priors(training_spam[:, 0])\n",
    "print(\"result\", log_class_priors)\n",
    "\n",
    "# Check length\n",
    "assert(len(log_class_priors) == 2)\n",
    "\n",
    "# Check whether the returned object is a numpy.ndarray\n",
    "assert(isinstance(log_class_priors, np.ndarray))\n",
    "\n",
    "# Check whether the values of this numpy.array are floats.\n",
    "assert(log_class_priors.dtype == float)\n",
    "\n",
    "# Check whether the values are both negative (the logarithm of a probability 0 < p < 1 should be negative).\n",
    "assert(np.all(log_class_priors < 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-footwear",
   "metadata": {},
   "source": [
    "## Part B: Estimate class-conditional likelihoods\n",
    "Define a function called `estimate_log_class_conditional_likelihoods()` that takes as input a data set with input features and with binary response variable (0s and 1s) and returns the **logarithm** of the empirical class-conditional likelihoods $\\log \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)$ for all words $w_i$ and both classes ($c \\in {0, 1}$). These parameters should be returned in a two-dimensional numpy-array with shape = `[num_classes, num_features]`.\n",
    "\n",
    "Assume a multinomial event model and use Laplace smoothing with $\\alpha = 1$. \n",
    "\n",
    "Hint: many `numpy`-functions contain an `axis` argument. If you specify `axis=0`, you can perform column-wise (that is, feature-wise!) computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "material-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_log_class_conditional_likelihoods(input_data, labels, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Given input_data of binary features (words) and labels \n",
    "    (binary response variable (0s and 1s)), calculate the logarithm \n",
    "    of the empirical class-conditional likelihoods, that is,\n",
    "    log(P(w_i | c)) for all features w_i and both classes (c in {0, 1}).\n",
    "\n",
    "    Assume a multinomial feature distribution and use Laplace smoothing\n",
    "    if alpha > 0.\n",
    "\n",
    "    :param input_data: a two-dimensional numpy-array with shape = [n_samples, n_features]\n",
    "                       contains binary features (words)\n",
    "    :param labels: a numpy array of length n_samples \n",
    "                   contains response variable\n",
    "\n",
    "    :return theta:\n",
    "        a numpy array of shape = [2, n_features]. theta[j, i] corresponds to the\n",
    "        logarithm of the probability of feature i appearing in a sample belonging \n",
    "        to class j.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE...\n",
    "    theta = np.zeros((2, input_data.shape[1]))\n",
    "    for i in range(2):\n",
    "        theta[i] = np.log((np.sum(input_data[labels == i], axis=0) + alpha) / (np.sum(labels == i) + alpha * input_data.shape[1]))\n",
    "\n",
    "    \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "matched-moldova",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0a02e324d633b49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.82996121 -2.39191618 -1.36699161 -5.80964287 -1.56114762 -2.34390696\n",
      "  -3.93784069 -2.78921798 -2.513806   -1.87781723 -2.86520389 -0.92684094\n",
      "  -2.10834089 -3.17058554 -4.0178834  -2.39191618 -2.32840278 -2.10834089\n",
      "  -0.66214839 -3.93784069 -1.15568252 -4.71103058 -3.6695767  -3.93784069\n",
      "  -1.05605267 -1.35529557 -1.40292362 -1.93844185 -2.10834089 -1.99193054\n",
      "  -2.37565566 -2.63158904 -2.28328234 -2.61096975 -1.94891315 -1.86806106\n",
      "  -1.33230605 -4.0178834  -2.2542948  -2.39191618 -3.17058554 -2.17205671\n",
      "  -2.32840278 -2.37565566 -1.30429301 -1.8488297  -4.0178834  -2.89187213\n",
      "  -1.77540223 -0.67384443 -2.00298038 -1.3210065  -2.34390696 -2.53249813]\n",
      " [-1.09861229 -1.29325433 -0.6423075  -4.14313473 -0.55171061 -1.00764051\n",
      "  -1.02644984 -1.14740245 -1.33545468 -0.87410912 -1.33545468 -0.59598343\n",
      "  -1.26073114 -2.19722458 -2.08171169 -0.69088217 -0.95324644 -1.17638999\n",
      "  -0.2368424  -1.61170806 -0.33647224 -3.1446059  -1.24485779 -1.03918887\n",
      "  -3.60413823 -3.8918203  -5.39589769 -4.29728541 -4.47960696 -4.70275051\n",
      "  -5.39589769 -6.08904488 -3.52409552 -6.08904488 -3.31645615 -2.75684037\n",
      "  -2.91099105 -4.14313473 -3.25583153 -2.11875296 -6.08904488 -4.99043259\n",
      "  -2.87016905 -4.47960696 -1.41621604 -3.52409552 -4.29728541 -4.70275051\n",
      "  -1.93016179 -0.57965654 -2.50552594 -0.2869265  -0.58778666 -1.41621604]]\n"
     ]
    }
   ],
   "source": [
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_conditional_likelihoods = estimate_log_class_conditional_likelihoods(\n",
    "    training_spam[:, 1:], training_spam[:, 0], alpha=1.0)\n",
    "print(log_class_conditional_likelihoods)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(log_class_conditional_likelihoods, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(log_class_conditional_likelihoods.shape == (2, 54))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(log_class_conditional_likelihoods.dtype == float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-removal",
   "metadata": {},
   "source": [
    "## Part C: Combine intermediate steps into train method\n",
    "Define a function called `train()` that takes as input a data set with binary response variable (0s and 1s) in the left-most column and returns both the **logarithm** of the empirical class priors $\\textcolor{green}{p(C=c)}$ for $c \\in \\{0, 1\\}$ and the **logarithm** of the empirical class-conditional likelihoods $\\log \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)$ for all words $w_i$ and both classes ($c \\in {0, 1}$). \n",
    "\n",
    "Having implemented `estimate_log_class_priors()` and `estimate_log_class_conditional_likelihoods` you just need to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "polish-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_data, labels, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Given input_data of binary features (words) and labels \n",
    "    (binary response variable (0s and 1s)), calculate \n",
    "    * the logarithm of the empirical class priors, that is, \n",
    "      the logarithm of the proportions of 0s and 1s:\n",
    "        log(p(C=0)) and log(p(C=1))\n",
    "    * the logarithm of the empirical class-conditional likelihoods, \n",
    "      that is, log(P(w_i | c)) for all features w_i and both classes (c in {0, 1}).\n",
    "\n",
    "    Assume a multinomial feature distribution and use Laplace smoothing\n",
    "    if alpha > 0.\n",
    "\n",
    "    :param input_data: a two-dimensional numpy-array with shape = [n_samples, n_features]\n",
    "    :param labels: a one-dimensional numpy-array with shape = [n_samples,]\n",
    "    :param alpha: a Laplace smoothing parameter\n",
    "\n",
    "    :return \n",
    "        log_class_priors: a numpy array of length two\n",
    "    \n",
    "        theta:\n",
    "        a numpy array of shape = [2, n_features]. theta[j, i] corresponds to the\n",
    "        logarithm of the probability of feature i appearing in a sample belonging \n",
    "        to class j.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE...\n",
    "    log_class_priors = estimate_log_class_priors(labels)\n",
    "    theta = estimate_log_class_conditional_likelihoods(input_data, labels, alpha)\n",
    "    return log_class_priors, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "literary-bidding",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eef6cd12dd6c4312",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log class priors\n",
      "[-0.48939034 -0.94933059]\n",
      "log class conditional likelihoods\n",
      "[[-1.82996121 -2.39191618 -1.36699161 -5.80964287 -1.56114762 -2.34390696\n",
      "  -3.93784069 -2.78921798 -2.513806   -1.87781723 -2.86520389 -0.92684094\n",
      "  -2.10834089 -3.17058554 -4.0178834  -2.39191618 -2.32840278 -2.10834089\n",
      "  -0.66214839 -3.93784069 -1.15568252 -4.71103058 -3.6695767  -3.93784069\n",
      "  -1.05605267 -1.35529557 -1.40292362 -1.93844185 -2.10834089 -1.99193054\n",
      "  -2.37565566 -2.63158904 -2.28328234 -2.61096975 -1.94891315 -1.86806106\n",
      "  -1.33230605 -4.0178834  -2.2542948  -2.39191618 -3.17058554 -2.17205671\n",
      "  -2.32840278 -2.37565566 -1.30429301 -1.8488297  -4.0178834  -2.89187213\n",
      "  -1.77540223 -0.67384443 -2.00298038 -1.3210065  -2.34390696 -2.53249813]\n",
      " [-1.09861229 -1.29325433 -0.6423075  -4.14313473 -0.55171061 -1.00764051\n",
      "  -1.02644984 -1.14740245 -1.33545468 -0.87410912 -1.33545468 -0.59598343\n",
      "  -1.26073114 -2.19722458 -2.08171169 -0.69088217 -0.95324644 -1.17638999\n",
      "  -0.2368424  -1.61170806 -0.33647224 -3.1446059  -1.24485779 -1.03918887\n",
      "  -3.60413823 -3.8918203  -5.39589769 -4.29728541 -4.47960696 -4.70275051\n",
      "  -5.39589769 -6.08904488 -3.52409552 -6.08904488 -3.31645615 -2.75684037\n",
      "  -2.91099105 -4.14313473 -3.25583153 -2.11875296 -6.08904488 -4.99043259\n",
      "  -2.87016905 -4.47960696 -1.41621604 -3.52409552 -4.29728541 -4.70275051\n",
      "  -1.93016179 -0.57965654 -2.50552594 -0.2869265  -0.58778666 -1.41621604]]\n"
     ]
    }
   ],
   "source": [
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_priors, log_class_conditional_likelihoods = train(\n",
    "    training_spam[:, 1:], training_spam[:, 0], alpha=1.0)\n",
    "print(\"log class priors\")\n",
    "print(log_class_priors)\n",
    "\n",
    "# Check length\n",
    "assert(len(log_class_priors) == 2)\n",
    "\n",
    "# Check whether the returned object is a numpy.ndarray\n",
    "assert(isinstance(log_class_priors, np.ndarray))\n",
    "\n",
    "# Check wehther the values of this numpy.array are floats.\n",
    "assert(log_class_priors.dtype == float)\n",
    "\n",
    "# Check wehther the values are both negative (the logarithm of a probability 0 < p < 1 should be negative).\n",
    "assert(np.all(log_class_priors < 0))\n",
    "\n",
    "print(\"log class conditional likelihoods\")\n",
    "print(log_class_conditional_likelihoods)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(log_class_conditional_likelihoods, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(log_class_conditional_likelihoods.shape == (2, 54))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(log_class_conditional_likelihoods.dtype == float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-compiler",
   "metadata": {},
   "source": [
    "## Part  D: Classify e-mails\n",
    "\n",
    "Having calculated the log class priors and the log class-conditional likelihoods for a given training set, define a function called `predict()`that takes a data set of new messages as input and predicts for each message whether it is spam or not. Note that the input should **not** contain a response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vulnerable-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(new_data, log_class_priors, log_class_conditional_likelihoods):\n",
    "    \"\"\"\n",
    "    Given a new data set with binary features, predict the corresponding\n",
    "    response for each instance (row) of the new_data set.\n",
    "\n",
    "    :param new_data: a two-dimensional numpy-array with shape = [n_test_samples, n_features].\n",
    "    :param log_class_priors: a numpy array of length 2.\n",
    "    :param log_class_conditional_likelihoods: a numpy array of shape = [2, n_features].\n",
    "        theta[j, i] corresponds to the logarithm of the probability of feature i appearing\n",
    "        in a sample belonging to class j.\n",
    "    :return class_predictions: a numpy array containing the class predictions for each row\n",
    "        of new_data.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE...\n",
    "\n",
    "    class_predictions = np.zeros(new_data.shape[0])\n",
    "    for i in range(new_data.shape[0]):\n",
    "        class_predictions[i] = np.argmax(np.array([np.sum(new_data[i] * log_class_conditional_likelihoods[0]) + log_class_priors[0], np.sum(new_data[i] * log_class_conditional_likelihoods[1]) + log_class_priors[1]]))\n",
    "\n",
    "    \n",
    "    return class_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "recovered-dining",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77f96654132ddd809aff99ccad684992",
     "grade": false,
     "grade_id": "cell-4c8adaa150209180",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "class_predictions = predict(training_spam[:, 1:], log_class_priors, log_class_conditional_likelihoods)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(class_predictions, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(class_predictions.shape == (1000,))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(np.all(np.logical_or(class_predictions == 0, class_predictions == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-mother",
   "metadata": {},
   "source": [
    "Now test your `predict` function by classifying messages. You can do this to the *training* data, but you should also try it on the *testing* data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "designing-fancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.814\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy\n",
    "true_classes = training_spam[:, 0]\n",
    "training_set_accuracy = np.mean(np.equal(class_predictions, true_classes))\n",
    "print(f\"Accuracy on the training set: {training_set_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-stadium",
   "metadata": {},
   "source": [
    "Once you are done, you can move the code into the main assignment notebook.\n",
    "\n",
    "One way to do this is to follow the rough structure of the class that already exists in that notebook. You can use the `train` method to pass in the data and perform all of the steps before the prediction. Note that you may need to adjust the code from this notebook. For example, you should store data in instance variables, e.g. `self.log_class_priors` and `self.log_class_conditional_likelihoods`. This means that then you can set up the `predict` method to match the one above without needing to pass in the additional variables. **Important:** the predict method in the classification.ipynb notebook must only take a single variable as a parameter (the one called `new_data`) in the skeleton code above."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
